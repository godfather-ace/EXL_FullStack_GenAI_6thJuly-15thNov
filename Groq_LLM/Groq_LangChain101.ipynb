{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### langChain's ChatGroq Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model = \"llama3-8b-8192\", \n",
    "    temperature = 0.1, #degree of factuality(towards 0) or randomness (towards 1) \n",
    "    max_tokens = None, #maximum amount of tokens that will be generated\n",
    "    timeout= None, #request timeout\n",
    "    max_retries = 1, # maximum retries for getting response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Large Language Models (LLMs), Transformer models refer to a type of neural network architecture designed specifically for natural language processing tasks. It's called \"Transformer\" because it uses self-attention mechanisms to transform input sequences into output sequences. This architecture is particularly effective for tasks like language translation, text summarization, and question answering. The Transformer model consists of an encoder and a decoder, where the encoder takes in input sequences and the decoder generates output sequences. This architecture has revolutionized the field of NLP, achieving state-of-the-art results in many tasks.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\", \n",
    "        \"You are a helpful assistant that gives explanations based on questions asked.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", \n",
    "        \"What is the meaning of Transformer models in LLMs? Explain in less than 100 words.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "res = llm.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism in Transformer models is a key component that allows the model to focus on specific parts of the input sequence when computing the output. It's a weighted sum of the input sequence, where the weights are learned during training. The attention weights are calculated by comparing the input sequence with a query vector, and the output is a weighted sum of the input sequence based on these weights. This allows the model to selectively attend to different parts of the input sequence, which is particularly useful for long-range dependencies and parallelization.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are a helpful assistant that gives explanations based on questions asked.\"\n",
    "\n",
    "        ), \n",
    "        (\n",
    "            \"human\", \"{input}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm \n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\", \n",
    "        \"output_language\": \"English\", \n",
    "        \"input\": \"Explain attention mechanism in Transformer models within 100 words.\"\n",
    "    }\n",
    ")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
